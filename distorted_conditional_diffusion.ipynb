{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6ze0ihNm1t8ElK0P64XXs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucyvost/distorted_diffusion/blob/main/distorted_conditional_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install dependencies\n",
        "!pip install rdkit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL4oKmxeUYHK",
        "outputId": "d2107f91-ae14-43cd-ea30-30e04263288e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (10.4.0)\n",
            "Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "import numpy as np\n",
        "import random\n",
        "\n"
      ],
      "metadata": {
        "id": "g2salRc0Wv4Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load any 3D molecular dataset in dictionary form as taken in by EDM."
      ],
      "metadata": {
        "id": "Lv_1ulcMRiZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "jTBj8ICiRdWd",
        "outputId": "27cb2149-dd97-418a-eb91-878d48751d25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/e3_diffusion_for_molecules\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/e3_diffusion_for_molecules/qm9/data/prepare/process.py:126: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if line[0] is '#':\n",
            "/content/e3_diffusion_for_molecules/qm9/data/prepare/process.py:128: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if line_counter is 0:\n",
            "/content/e3_diffusion_for_molecules/qm9/data/prepare/process.py:130: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif line_counter is 1:\n",
            "/content/e3_diffusion_for_molecules/qm9/data/prepare/process.py:143: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if len(split) is 4:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'qm9/train.npz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e5bd87d82bf5>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{dataset}/train.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{dataset}/test.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{dataset}/valid.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'qm9/train.npz'"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "train = np.load(f'content/train.npz')\n",
        "test = np.load(f'content/test.npz')\n",
        "valid = np.load(f'content/valid.npz')\n",
        "\n",
        "# Create a new npz file to combine the data\n",
        "combined_file = f'all_data.npz'\n",
        "combined_data = {}\n",
        "# Iterate through keys and combine the data\n",
        "for key in test.keys():\n",
        "    combined_data[key] = np.concatenate([train[key], test[key], valid[key]], axis=0)\n",
        "\n",
        "# Save the combined data to a new npz file\n",
        "#np.savez(combined_file, **combined_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Add some distorted molecules to the dataset"
      ],
      "metadata": {
        "id": "2w4Q6XXTXXth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scramble_coordinates_3d(coordinates, max_scramble=0.25):\n",
        "    \"\"\"\n",
        "    Scramble a list of 3D coordinates by a random amount between 0 and max_scramble.\n",
        "\n",
        "    Parameters:\n",
        "    - coordinates: A list of tuples, each containing (x, y, z) coordinates.\n",
        "    - max_scramble: The maximum amount to scramble (0 to 1).\n",
        "\n",
        "    Returns:\n",
        "    - A new list of scrambled 3D coordinates.\n",
        "    \"\"\"\n",
        "    scrambled_coordinates = []\n",
        "    for x, y, z in coordinates:\n",
        "        # Generate random offsets within the specified range for each dimension\n",
        "        offset_x = random.uniform(-max_scramble, max_scramble)\n",
        "        offset_y = random.uniform(-max_scramble, max_scramble)\n",
        "        offset_z = random.uniform(-max_scramble, max_scramble)\n",
        "\n",
        "        # Apply the offsets to the coordinates\n",
        "        scrambled_x = x + offset_x\n",
        "        scrambled_y = y + offset_y\n",
        "        scrambled_z = z + offset_z\n",
        "\n",
        "        scrambled_coordinates.append([scrambled_x, scrambled_y, scrambled_z])\n",
        "\n",
        "    return np.array(scrambled_coordinates)"
      ],
      "metadata": {
        "id": "1BaDeIdQXd3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "#new distorted version\n",
        "conditional_dict = {}\n",
        "#load all the data first so we don't need to re-load the whole dictionary every time\n",
        "all_positions = combined_data['positions']\n",
        "all_charges = combined_data['charges']\n",
        "for idx, mol in tqdm(enumerate(all_charges), total=len(all_charges)):\n",
        "\n",
        "\n",
        "    conditional_dict['positions'].append(all_positions[idx])\n",
        "    conditional_dict['charges'].append(all_charges[idx])\n",
        "    conditional_dict['scramble'].append(0)\n",
        "    conditional_dict['num_atoms'].append(len(np.where(mol != 0 )))\n",
        "\n",
        "    #one slightly messed up version\n",
        "    if random.random() < 0.02:\n",
        "\n",
        "\n",
        "        conditional_dict['num_atoms'].append(len(np.where(mol != 0 ))\n",
        "        slight_mess = random.uniform(0, 0.25)\n",
        "        conditional_dict['charges'].append(all_charges[idx])\n",
        "        conditional_dict['scramble'].append(slight_mess)\n",
        "        new_coords = scramble_coordinates_3d(new_positions,max_scramble=slight_mess)[0:num_atoms]\n",
        "\n",
        "        new_coords = np.append(new_coords, np.zeros([to_pad,3])).reshape((100,3))\n",
        "\n",
        "        conditional_dict['positions'].append(new_coords)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "DaPOzmexd00p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle indices and split data\n",
        "shuffled_indices = random.sample(range(len(new_data['charges'])), k=len(new_data['charges']))\n",
        "split_points = lambda n: [int(np.floor(n * x)) for x in [0.66, 0.76, 0.86]]\n",
        "\n",
        "# Extract precomputed data\n",
        "positions_data, charges_data, scramble_data, num_atoms_data = map(np.array,\n",
        "    (new_data['positions'], new_data['charges'], new_data['scramble'], new_data['num_atoms']))\n",
        "\n",
        "# Define split indices\n",
        "half_train_end, half_test_start, half_valid_start = split_points(len(new_data['charges']))\n",
        "\n",
        "# Function to create dictionaries\n",
        "def create_dict(indices):\n",
        "    return {\n",
        "        'positions': positions_data[indices],\n",
        "        'charges': charges_data[indices],\n",
        "        'scramble': scramble_data[indices],\n",
        "        'num_atoms': num_atoms_data[indices]\n",
        "    }\n",
        "\n",
        "# Create train, test, and valid dictionaries\n",
        "train_dict = create_dict(shuffled_indices[:half_train_end])\n",
        "test_dict = create_dict(shuffled_indices[half_test_start:])\n",
        "valid_dict = create_dict(shuffled_indices[half_valid_start:])\n",
        "\n",
        "# Save datasets\n",
        "\n",
        "for split, data in zip(['train_dist', 'test_dist', 'valid_dist'], [train_dict, test_dict, valid_dict]):\n",
        "    np.savez(f'{split}.npz', **data)\n"
      ],
      "metadata": {
        "id": "7MsV6NEK_gq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you now have a set of dictionaries you can use to train a conditional model on the 'scramble' property!"
      ],
      "metadata": {
        "id": "2flmIV_w_X1f"
      }
    }
  ]
}